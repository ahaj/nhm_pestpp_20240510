{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1426a5c",
   "metadata": {},
   "source": [
    "### This notebook subsets the NHM CONUS baseline data used for calibration targets to create observation files (.nc) for each model extraction in the root folder. These created files will be read by the subsequent Notebook to make files used (read during) in PEST++ calibration.\n",
    "#### This notebook also preprocesses the SCA baseline data to emulate the filtering that is done in NHM calibration with Fortran.\n",
    "#### This only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf95cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywatershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461b850",
   "metadata": {},
   "source": [
    "### define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759528db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['01473000', '05431486','09112500','14015000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = pl.Path('../NHM_extractions/20230110_pois_haj/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b0b6e",
   "metadata": {},
   "source": [
    "### make observations dirs in each extraction directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13700147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm in all_models:\n",
    "    if not (rootdir / cm / 'observation_data').exists():\n",
    "        (rootdir / cm / 'observation_data').mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725e3b1",
   "metadata": {},
   "source": [
    "### now grab all the `nhm_ids` from the `myparam.param` file for each cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm_ids = dict(zip(all_models, \n",
    "        [pywatershed.parameters.PrmsParameters.load(rootdir / cm / 'myparam.param').parameters['nhm_id'] for cm in all_models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716290d",
   "metadata": {},
   "source": [
    "### assign `wkdir` to indicate where the raw CONUS netCDF files live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "wkdir = pl.Path('../Supporting_information/CONUS_baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lu = pd.read_csv('../Supporting_information/target_and_output_vars_table.csv', index_col=0)\n",
    "lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in wkdir.glob('*.nc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db988a3",
   "metadata": {},
   "source": [
    "### Slice output to calibration periods for each variable\n",
    "#### These are as follows from table 2 (Hay and others, 2023):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dba819",
   "metadata": {},
   "outputs": [],
   "source": [
    "aet_start = '2000-01-01'\n",
    "aet_end = '2010-12-31'\n",
    "recharge_start = '2000-01-01'\n",
    "recharge_end = '2009-12-31'\n",
    "runoff_start = '1982-01-01'\n",
    "runoff_end = '2010-12-31'\n",
    "soil_rechr_start = '1982-01-01'\n",
    "soil_rechr_end = '2010-12-31'\n",
    "sca_start = '2000-01-01'\n",
    "sca_end = '2010-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0538e98",
   "metadata": {},
   "source": [
    "### Subset AET NHM baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "AET_all = xr.open_dataset(wkdir / 'baseline_AET_v11.nc')\n",
    "#AET_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12329a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = AET_all.sel(nhru=c_ids)\n",
    "    c_da[['aet_max','aet_min']].to_netcdf(rootdir / cm / 'observation_data' / f'AET_monthly.nc')\n",
    "    c_da.groupby('time.month').mean().to_netcdf(rootdir / cm / 'observation_data' / f'AET_mean_monthly.nc')\n",
    "AET_all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909c8bc",
   "metadata": {},
   "source": [
    "###  Subset HRU Streamflow (RUNOFF NHM) baseline data--The MWBM term, \"runoff\" is total contribution to streamflow from each HRU. We are re-terming this in the subset file to \"hru_streamflow\" to clearly describe HRU contributions to streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_all = xr.open_dataset(wkdir / 'baseline_RUN_v11.nc')\n",
    "#RUN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e207c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = RUN_all.sel(nhru=c_ids, time=slice(runoff_start , runoff_end))\n",
    "    c_da[['runoff_mwbm','runoff_min', 'runoff_max']].to_netcdf(rootdir / cm / 'observation_data' / f'hru_streamflow_monthly.nc')\n",
    "RUN_all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b98ea",
   "metadata": {},
   "source": [
    "### Subset Annual Recharge\n",
    "### These annual values are actually the average daily rate; and, match the units of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RCH_all = xr.open_dataset(wkdir / 'baseline_RCH_v11.nc')\n",
    "RCH_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = RCH_all.sel(nhru=c_ids)\n",
    "    c_da[['recharge_min_norm','recharge_max_norm']].to_netcdf(rootdir / cm / 'observation_data' / f'RCH_annual.nc')\n",
    "RCH_all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18be26",
   "metadata": {},
   "source": [
    "### Subset Annual Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0012ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOM_ann_all = xr.open_dataset(wkdir / 'baseline_SOMann_v11.nc')\n",
    "SOM_ann_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = SOM_ann_all.sel(nhru=c_ids)\n",
    "    c_da[['soil_moist_min_norm','soil_moist_max_norm']].to_netcdf(rootdir / cm / 'observation_data' / f'Soil_Moisture_annual.nc')\n",
    "SOM_ann_all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587903ae",
   "metadata": {},
   "source": [
    "### Subset Monthly Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269da33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOM_mon_all = xr.open_dataset(wkdir / 'baseline_SOMmth_v11.nc')\n",
    "SOM_mon_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451af385",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = SOM_mon_all.sel(nhru=c_ids)\n",
    "    c_da[['soil_moist_min_norm','soil_moist_max_norm']].to_netcdf(rootdir / cm / 'observation_data' / f'Soil_Moisture_monthly.nc')\n",
    "SOM_mon_all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a16f8",
   "metadata": {},
   "source": [
    "### Subset and pre-process Daily Snow Covered Area\n",
    "#### checked with Parker on 4/18/23 and script appears to function as intended. Also added more explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data set. Lauren Hay developed fortran code embedded in the NHM that pre-processed the raw data,\n",
    "# applying several filters.\n",
    "SCA= xr.open_dataset(wkdir / 'baseline_SCA_v11.nc')\n",
    "#SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367547f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating variables used in Parker Norton's function.\n",
    "baseline_file = wkdir / 'baseline_SCA_v11.nc'\n",
    "sca_var = 'snow_cover_extent'\n",
    "ci_var = 'sca_clear_index'\n",
    "#st_date = '2000-01-01' #per publication\n",
    "#en_date = '2010-12-31' #per publication\n",
    "remove_ja = True #This is technically the first filter for removing July and August from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filename, f_vars, start_date, end_date):\n",
    "    # This routine assumes dimension nhru exists and variable nhm_id exists\n",
    "    df = xr.open_dataset(filename)\n",
    "    # NOTE: Next line needed if nhm_id variable exists in netcdf file\n",
    "    df = df.assign_coords(nhru=df.nhm_id)\n",
    "    if isinstance(f_vars, list):\n",
    "     df = df[f_vars].sel(time=slice(start_date, end_date))\n",
    "    else:\n",
    "     df = df[[f_vars]].sel(time=slice(start_date, end_date))\n",
    "    return df\n",
    "\n",
    "baseline_df = get_dataset(baseline_file, [sca_var, ci_var, 'nhru'], sca_start, sca_end) \n",
    "\n",
    "#Applying first filter to remove selected months, July and August, from the dataset, selects months to keep.\n",
    "if remove_ja:\n",
    "    # \n",
    "    baseline_restr = baseline_df.sel(time=baseline_df.time.dt.month.isin([1, 2, 3, 4, 5, 6, 9, 10, 11, 12]))\n",
    "else:\n",
    "    baseline_restr = baseline_df\n",
    "baseline_df.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fece70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SCAmask to remove other data meeting criteria below.\n",
    "\n",
    "# Compute lower and upper SCA values based on confidence interval(used to be called the clear index). Comes from MODIS,\n",
    "# \"fraction of the cell observed in cloud free conditions,\" here, if cloud cover is less than 30%, then,\n",
    "# the SCA values is used; \n",
    "threshold = 70.0\n",
    "ci_pct = baseline_restr[ci_var].where(baseline_restr[ci_var] >= threshold)\n",
    "ci_pct /= 100.0\n",
    "\n",
    "# Mask SCA values where CI is masked; this included daily targets for HRUs when the clear index was greater than 70%\n",
    "sca_obs = baseline_restr[sca_var].where(~np.isnan(ci_pct))\n",
    "\n",
    "# Maximum SCA value of those within the threshold...so really \"sca_obs_max\"\n",
    "msk_SCAmax = sca_obs.max(axis=0)\n",
    "\n",
    "# Now count the data sca_obs:\n",
    "# Number of daily values > 0.0 by HRU\n",
    "msk_num_obs = (sca_obs > 0.0).sum(axis=0)\n",
    "\n",
    "#Excluding HRUs that do not have enough values\n",
    "#Number of years of values by HRU: How many years of annula values that are greater than 0?\n",
    "msk_num_ann = sca_obs.resample(time='1AS').mean() # resamples the df and finds the average value for each year\n",
    "msk_num_ann = (msk_num_ann > 0).sum(axis=0) # takes a count of all average annual values greater than 0.\n",
    "\n",
    "# Create SCA mask based on:\n",
    "# 1 - Keeps HRUs targets where at least 2 years of data that were the annual average values are greater than 0 (see above),\n",
    "# 2 - and, where sca_max is greater than 50%, \n",
    "# 3 - and, where there are least 9 days of values in the total selected period.\n",
    "SCAmask = (msk_num_ann > 1) & (msk_SCAmax > 0.5) & (msk_num_obs > 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower bound of SCA by HRU\n",
    "baseline_SCAmin = (ci_pct * sca_obs).where(SCAmask)# Computes min based upon %SCA of the %area visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper bound of SCA by HRU\n",
    "baseline_SCAmax = (baseline_SCAmin + (1.0 - ci_pct)).where(SCAmask)# Computes max based upon % SCA + %area not visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCA_daily = xr.combine_by_coords([baseline_SCAmin.to_dataset(name='SCA_min'), baseline_SCAmax.to_dataset(name='SCA_max')])\n",
    "SCA_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bca7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm, c_ids in nhm_ids.items():\n",
    "    c_da = SCA_daily.sel(nhru=c_ids)\n",
    "    c_da.to_netcdf(rootdir / cm / 'observation_data' / f'SCA_daily.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32320252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCA.close()\n",
    "SCA_daily.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7b5db-2e94-45de-be96-398f18321407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
